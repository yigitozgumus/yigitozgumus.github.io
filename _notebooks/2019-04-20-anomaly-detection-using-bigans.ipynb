{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Anomaly Detection Using Generative Adversarial Networks\"\n",
    "> \"My thoughts about anomaly detection using GAN's\"\n",
    "\n",
    "- toc: false\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [thesis, gan, deep learning]\n",
    "- image: \"./images/anomaly/header.png\"\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post I will explain the architecture of the bigan and how can it be used for the anomaly detection problem. The \n",
    "papers that inspired this post are down below at the references section.\n",
    "\n",
    "# What is Anomaly Detection ?\n",
    "\n",
    "Anomaly detection is one of the most important problems concerning multiple domains including manufacturing, \n",
    "Cyber-security, fraud detection and medical imaging. At its core an Anomaly Detection method should learn the data \n",
    "distribution of the normal samples which can be complex and high dimensional to identify the anomalous ones. \n",
    "\n",
    "The method I will explain focuses on the reconstruction based approach to indentify the anomalous samples. By learning the \n",
    "data distribution and its representation, model is then able to reconstruct a sample image that is similar to the input for \n",
    "the inference. By defining a score function to measure the similarity between the input image and the reconstructed output, \n",
    "we can attain a score that can be used to identify a sample as anomalous or normal. Since the model is trained with the normal \n",
    "images, ideally, when the test image is normal, the reconstructed sample is expected to obtain a lower anomaly score compared to \n",
    "an anomalous image. That is the basis for the anomaly detection using reconstruction based approach.\n",
    "\n",
    "Generative Adversarial Networks, or GANs are considered as a significant breakthrough in deep \n",
    "learning. They are used to model complex and high dimensional distributions using adversarial training. Let's explore \n",
    "what that means and how we can use it \n",
    "for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition Behind GANs \n",
    "\n",
    " Generative Adversarial Networks consist of 2 networks, one generator and one discriminator. The generator is responsible \n",
    " for generating sample images that is similar to the dataset samples and tries to fool the discriminator. The purpose of \n",
    " the discriminator is to identify whether the image is from the dataset or it is generated by the generator, in other words to \n",
    " classify input images as \"real\" or \"fake\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The pipeline of GAN Framework](images/anomaly/gan.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training idea behind GANs is based on game theory and assuming that the two network are competing each other. \n",
    "The model is usually trained with the gradient-based approaches by taking minibatch of fake images generated by transforming \n",
    "random vectors sampled from $p_z(z)$ via the generator and minibatch of data samples from $p_{data}(x)$. They are \n",
    "used to maximize $V(D, G)$ with respect to parameters of $D$ by assuming a\n",
    "constant $G$, and then minimizing $V(D, G)$ with respect to parameters of $G$ by assuming a constant $D$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\normalsize\n",
    "\\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x; \\theta_d})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z; \\theta_g})))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 2 loops and two terms. Let's disect each term. They will provive useful for the BiGAN model in the \n",
    " next section.\n",
    "\n",
    "## Term 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    D(\\boldsymbol{x ; \\theta_d}) &\\rightarrow \\text{Likelihood of Discriminator identifying x as Real}\\\\\n",
    "    \\\\\n",
    "    \\log D(\\boldsymbol{x ; \\theta_d}) &\\rightarrow \\text{Log Likelihood of Discriminator identifying x as Real} \\\\\n",
    "    \\\\\n",
    "    \\mathbb{E}_{\\boldsymbol{x; \\theta_d} \\sim p_{\\text { data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x; \\theta_d})] &\\rightarrow \\text{Expected Log Likelihood of input samples from real data} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    G(\\boldsymbol{z ; \\theta_g}) &\\rightarrow \\text{Generated image sample from noise $z$} \\\\\n",
    "    \\\\\n",
    "    D(G(\\boldsymbol{z ; \\theta_g}); \\theta_d) &\\rightarrow \\text{The likelihood of the image from Generator being real} \\\\\n",
    "    \\\\\n",
    "    \\log D(G(\\boldsymbol{z ; \\theta_g}); \\theta_d) &\\rightarrow \\text{The log likelihood of the image from Generator being real} \\\\\n",
    "    \\\\\n",
    "    \\log (1-D(G(\\boldsymbol{z ; \\theta_g}); \\theta_d)) &\\rightarrow \\text{The log likelihood of the image from Generator being fake} \\\\\n",
    "    \\\\ \n",
    "    \\mathbb{E}_{\\boldsymbol{z } \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z ; \\theta_g}); \\theta_d))] &\\rightarrow \\text{E.L.L of Discriminator identifying generated image as fake}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are two main loops in the equation that we need to examine. The inner loop of the discriminator and the \n",
    "outer loop of the Generator. \n",
    "If want to summarize their logic: \n",
    "\n",
    "Inner loop is the training objective of the discriminator. It wants to maximize its probability of \n",
    "classifying an image as real or fake. So It needs to maximize $D(x)$ to classify real images ( since $D(x)$ is the \n",
    "probability of a discriminator identifying image as real) and it also needs to maximize $(1 - D(G(z))$ to maximize its \n",
    "probability for spotting fake images. \n",
    "\n",
    "The outer loop is the training objective of the generator. Since it's only on one of the terms we don't \n",
    "need to look at the first term. Generator wants to fool the discriminator by generating more real like sample images. So \n",
    "in order for it to maximize its probability to get classified as real, it needs to minimize discriminator's probability \n",
    "of classifying generated image as fake. So it needs to minimize $D(G(z))$ to increase $(1 - D(G(z)))$.\n",
    "\n",
    "Now that's out of the way we can focus on what is BiGAN and how does it work ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiGAN Architecture \n",
    "\n",
    "BiGAN is composed of a standard GAN with an additional Encoder that is simultaneously trained with the generator and discriminator.\n",
    "This encoder learns the mapping representation from the input image to the latent space representation (noise). This approach enables\n",
    "inference process much faster than the previous proposed method which is an iterative optimization process via backpropagation for \n",
    "each sample. \n",
    "\n",
    "![BiGAN Framework](images/anomaly/bigan.jpg)\n",
    "\n",
    "With the addition of the Encoder, the Discriminator behavior changes a little. Now the discriminator not only discriminates in data\n",
    "space ($z$ or $G(z)$) but jointly in data and latent space tuples ($x$, $E(x)$) versus ($G(z)$ , $z$). Generator and encoder are trying \n",
    "to fool the discriminator. \n",
    "</p>\n",
    "\n",
    "Let's explain the objective function of the BiGAN like we did with GAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\min _{G,E} \\max _{D} V(G, D, E)=\\mathbb{E}_{x \\sim p_{x}}[\\log D(x, E(x))]+\\mathbb{E}_{z \\sim p_{z}}[\\log (1-D(G(z), z))]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our objective function is similar to GAN with the difference of the noise tuple from the encoder and the latent space\n",
    "distribution. From the discriminator's perspective, the pair with input image and the encoded noise of the input image \n",
    "should be classified as real, and the tuple with sampled noise and the generated image should be classified as fake. \n",
    "So again, it needs to maximize the probability of discriminating $(x, E(x))$ \n",
    "tuple as real and $(G(z), z)$ tuple as fake.\n",
    "\n",
    "We can consider the encoder and generator loss in the same loop because they are both trying to fool the discriminator. \n",
    "Encoder wants to minimize discriminator's probability of classifiying $(x, E(x))$. The reason for this is that in order \n",
    "for encoder to be an optimal one, it needs to learn the invert the input from the true data distribution, to act as \n",
    "$E = G^{-1}$. Generator again wants to minimize Discriminator's ability to spot a fake image so consequentially it wants to \n",
    "maximize $D(G(z), z)$ probability.\n",
    "\n",
    "\n",
    "After the training of the model is done, we can make an inference to test the model. The score function $A(x)$ is composed of the \n",
    "combination of the reconstruction loss ($L_G$) and discrimination-based loss ($L_D$). The score function and its components are depicted\n",
    "below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "A(x) &= \\alpha L_{G}(x)+(1-\\alpha) L_{D}(x) \\\\\n",
    "\\\\\n",
    "L_{G}(x) &= \\|x-G(E(x))\\|_{1} \\\\\n",
    "\\\\\n",
    "L_{D_{1}} &= \\sigma(D(x, E(x)), 1) \\\\\n",
    "\\\\\n",
    "L_{D_{2}} &= \\left\\|f_{D}(x, E(x))-f_{D}(G(E(x)), E(x))\\right\\|_{1} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reconstruction loss measures the difference between the input image and the reconstructed image. \n",
    "There are 2 types of discrimination loss that we can define. First one depends on the \n",
    "sigmoid cross entropy loss from the discriminator of x \n",
    "being a real example (class 1 in this case), and the second method for defining the discriminator loss is based on \n",
    "the <em>feature matching</em> loss with $f_D$ returning the layer preceding the logits for the given inputs in the discriminator. \n",
    "This loss evaluates if the reconstructed data has similar features in the discriminator as the true sample. \n",
    "Samples with larger $A(x)$ values are considered as more likely from the \n",
    "anomalous sample. \n",
    "\n",
    "In this introductory post I wanted to talk about the BiGAN architecture for the anomaly detection task. \n",
    "In the following posts I will talk more about anomaly detection, different architectures that can be used with more \n",
    "detailed summary of the architectures and the training techniques.\n",
    "\n",
    "# References\n",
    "\n",
    "* [Adversarial Feature Learning](https://arxiv.org/abs/1605.09782)\n",
    "* [Adversarially Learned Interface](https://arxiv.org/abs/1606.00704)\n",
    "* [Efficient GAN Based Anormaly Detection](https://arxiv.org/abs/1802.06222)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
